{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2dd0198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "from requests import request, get, post\n",
    "from requests.compat import urljoin, urlparse, urlunparse , quote, unquote\n",
    "from requests.exceptions import HTTPError\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from html import unescape\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "import json\n",
    "import requests\n",
    "from requests import Session\n",
    "from requests import post\n",
    "from requests.compat import urlencode\n",
    "from requests import Session\n",
    "from requests.cookies import cookiejar_from_dict\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import urllib.request\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c2d6e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentcreate(key,sort):\n",
    "    header={\"user-agent\" :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\"}\n",
    "    #헤더 필수\n",
    "\n",
    "    url = 'https://search.naver.com/search.naver?where=news' #뉴스 검색 url\n",
    "    \n",
    "\n",
    "    resp = request('GET', url, params={'sm':'tab_jum','query':key,'sort':sort}) #sort 0이면 관련도 순 1이면 최신순\n",
    "    \n",
    "\n",
    "    news_search = BeautifulSoup(resp.text, 'html.parser') #텍스트를 html로 전환\n",
    "    \n",
    "\n",
    "    def linkto(number):  #링크 태그 생성 함수\n",
    "        a='#sp_nws'\n",
    "        b=str(number)\n",
    "        c=' > div.news_wrap.api_ani_send > div > div.news_info > div.info_group > a:nth-child(3)'\n",
    "        return a+b+c\n",
    "   \n",
    "\n",
    "    def titleto(number):  #타이틀 태그 생성 함수\n",
    "        a='#sp_nws'\n",
    "        b=str(number)\n",
    "        c=' > div.news_wrap.api_ani_send > div > a'\n",
    "        return a+b+c\n",
    "    \n",
    "     \n",
    "    news_link2=[]  #빈리스트 생성\n",
    "    title2=[]     #빈리스트 생성\n",
    "    for count in range(1,13):\n",
    "        linktag=linkto(count)\n",
    "        titletag=titleto(count)\n",
    "        link=news_search.select_one(linktag)\n",
    "        title=news_search.select_one(titletag)\n",
    "        if link:\n",
    "            news_link2.append(link['href'])\n",
    "            title2.append(title['title'])\n",
    "\n",
    "#13번 돌려서 나온 기사 타이틀이랑 링크 리스트로 저장\n",
    "    \n",
    "\n",
    " \n",
    "    return news_link2, title2 #뉴스링크랑 제목 리스트 리턴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6f20a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titlecontent(news_link2):\n",
    "    content5=[] #빈리스트 생성\n",
    "\n",
    "    \n",
    "    for i in range(0,len(news_link2)):    \n",
    "        news = requests.get(news_link2[i],headers=header) #그 뉴스링크에 다시 접근\n",
    "        \n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\") #html로 변환\n",
    "\n",
    "        if news_link2[i].find('sports')>0:\n",
    "            content=news_html.find_all('div', {'id':'newsEndContents'})  ##스포츠 일때는 sid가 없어서 1순위 확인\n",
    "            content=str(content)\n",
    "            content=content.split('<p class=\"source\">') \n",
    "            content2=content[0]\n",
    "        elif news_link2[i][-3:]=='106':\n",
    "            content=news_html.find_all('div', {'id':'articeBody'})  ##엔터테이너 일때 sid = 106\n",
    "            content2= str(content)\n",
    "        else:\n",
    "            content = news_html.select(\"div#dic_area\")  ##일반뉴스일때 sid = 100~105 반례가 있을 수 있음 여기서\n",
    "            content2= str(content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def remove_image(content): #이미지 설명 제거\n",
    "            cleanr =re.compile('<em class=\"img_desc\">.*?</em>')\n",
    "            cleantext = re.sub(cleanr, '', content)\n",
    "            return cleantext\n",
    "\n",
    "        def remove_tag(content): #html태그 제거\n",
    "            cleanr =re.compile('<.*?>')\n",
    "            cleantext = re.sub(cleanr, '', content)\n",
    "            return cleantext\n",
    "\n",
    "        def remove_email(content): #이메일 제거\n",
    "            cleanr =re.compile('([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)')\n",
    "            cleantext = re.sub(cleanr, '', content)\n",
    "            return cleantext\n",
    "\n",
    "        content4=content2 #저장 혹시 모르니\n",
    "        content2= remove_image(content2) #이미지 설명 제거\n",
    "        content3= remove_email(content2)  #이메일 제거\n",
    "\n",
    "        content3=BeautifulSoup(content3) #다시 html로 \n",
    "        content3=content3.get_text() #텍스트만 추출 str\n",
    "        content3 ##본문\n",
    "\n",
    "\n",
    "        \n",
    "        content5.append(content3) #본문을 5에 리스트로 어펜드\n",
    "\n",
    "    return content5 #본문 리스트 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d26504c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_chrome_driver():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--lang=ko_KR')\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument('--window-size=1920x1080')\n",
    "\n",
    "    # headless임을 숨기기 위해서\n",
    "    # headless인 경우 Cloudflare 서비스가 동작한다.\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(\n",
    "        ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b00e2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mainkey(key,sort='0'):\n",
    "    news_link2,title2=contentcreate(key,sort) #함수 사용\n",
    "    content5 = titlecontent(news_link2) #함수 사용\n",
    "    con=\" \".join(content5) #띄어쓰기 기준으로 뉴스기사 합체\n",
    "    titlekeywords=con.split(' ') #띄어쓰기 기준으로 뉴스 키워드 분할\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        driver = set_chrome_driver()\n",
    "\n",
    "        url = 'https://namu.wiki/w/'+key\n",
    "        driver.get(url=url)\n",
    "        print(driver.title)\n",
    "        html = driver.page_source\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    imglink1=soup.select_one('tr:nth-child(2) > td > div > div > a > span > span > img.JGi8zLZN')\n",
    "\n",
    "\n",
    "\n",
    "    if imglink1: #있으면 이미지 링크 할당하고 끝\n",
    "        imglink=imglink1['src']\n",
    "    else: #없으면...\n",
    "        q1=soup.find_all('a', {'class':'x3PmqoJp'}) #이름 + 수식어 다 가져옴\n",
    "        qqq=[] #이름 수식어 들어갈 리스트\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,int(len(q1))):\n",
    "            if q1[i]['title'][0:3]==key: #이름으로 시작하는 이름 + 수식어만 가져옴\n",
    "                qqq.append(q1[i]['title'][3:]) #수식어 부분만 다시 추출\n",
    "\n",
    "        aaa = [0 for i in range(len(qqq))] #그만큼 빈 리스트\n",
    "\n",
    "        for i in range(0,len(qqq)):\n",
    "            if qqq[i] in titlekeywords: #위에서 띄어 쓰기 단위로 분할한 뉴스키워드에 수식어가 있으면 플러스 1\n",
    "                aaa[i]=aaa[i]+1\n",
    "\n",
    "\n",
    "        key2=qqq[aaa.index(max(aaa))] #가장 많은 빈도의 수식어 추출\n",
    "\n",
    "        key3=key+key2 #키 3에 새로운 키를 할당\n",
    "\n",
    "\n",
    "        if __name__ == \"__main__\": # 키 3로 다시돌림\n",
    "            driver = set_chrome_driver()\n",
    "\n",
    "            url = 'https://namu.wiki/w/'+key3\n",
    "            driver.get(url=url)\n",
    "            print(driver.title)\n",
    "            html = driver.page_source\n",
    "\n",
    "            driver.close()\n",
    "            soup = BeautifulSoup(html)\n",
    "            imglink1=soup.select_one('tr:nth-child(2) > td > div > div > a > span > span > img.JGi8zLZN')\n",
    "            imglink=imglink1['src']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    imglink # 생성된 이미지 링크\n",
    "\n",
    "    imglink2=\"https:\"+imglink\n",
    "    resp=requests.get(imglink2)\n",
    "    filename=key+',png'\n",
    "    with open(filename, 'wb') as f:\n",
    "                          f.write(resp.content)  ##해당키워드 이름으로 png 파일로 저장\n",
    "            \n",
    "            \n",
    "    return title2[0], content5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "89a145a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한지민 - 나무위키\n"
     ]
    }
   ],
   "source": [
    "key='한지민'\n",
    "sort='0'# 0이면 관련도순 1이면 최신순\n",
    "title,content=mainkey(key,sort)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
